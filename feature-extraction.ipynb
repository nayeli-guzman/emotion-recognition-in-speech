{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature extraction**\n",
    "\n",
    "To extract features from our dataset, which consists of the four previously mentioned datasets (`merging-datasets.ipynb`), we'll first apply **data augmentation**. After enhancing the dataset, we'll perform feature extraction using **ZCR, RMSE, and MFCC**.\n",
    "\n",
    "## **Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion_id</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>./archive/Actor_01/03-01-01-01-01-01-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>./archive/Actor_01/03-01-01-01-01-02-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>./archive/Actor_01/03-01-01-01-02-01-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>./archive/Actor_01/03-01-01-01-02-02-01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calm</td>\n",
       "      <td>./archive/Actor_01/03-01-02-01-01-01-01.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emotion_id                                    file_path\n",
       "0    neutral  ./archive/Actor_01/03-01-01-01-01-01-01.wav\n",
       "1    neutral  ./archive/Actor_01/03-01-01-01-01-02-01.wav\n",
       "2    neutral  ./archive/Actor_01/03-01-01-01-02-01-01.wav\n",
       "3    neutral  ./archive/Actor_01/03-01-01-01-02-02-01.wav\n",
       "4       calm  ./archive/Actor_01/03-01-02-01-01-01-01.wav"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Augmentation**\n",
    "\n",
    "This technique is used to increase the quantity and diversity of data in a training set, helping to improve the generalization capability of a ML model. In the case of audio, various transformations are applied to simulate natural variations in sound signals\n",
    "\n",
    "### **1. Noise**\n",
    "\n",
    "- Random noise is added to the audio signal to simulate recordings in noisy environments.\n",
    "- This helps the model become more robust to variations in recording conditions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stretch**\n",
    "\n",
    "- Changes the speed of the audio without altering its pitch.\n",
    "- Useful for simulating variations in the duration of words or phrases without affecting the frequency of the sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(y=data, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Shift (Time Shifting)**\n",
    "- Shifts the audio signal in time to generate modified versions of the same recording.\n",
    "- Helps make the model more robust to slight changes in the temporal alignment of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Pitch (Pitch Shifting)**\n",
    "- Alters the frequency of the audio signal without changing its duration.\n",
    "- Simulates variations in speakers' voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**\n",
    "\n",
    "### **Load packages**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Crossing Rate (**ZCR**), Root Mean Square Energy (**RMSE**), and Mel-Frequency Cepstral Coefficients (**MFCC**) are some of the feature extraction techniques used to analyze audio signals. These features help capture important characteristics of the sound, such as signal energy, spectral properties, and frequency variations, making them useful for tasks like speech recognition, emotion detection, and audio classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr(data, frame_length, hop_length):\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "def rmse(data, frame_length=2048, hop_length=512):\n",
    "    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "\n",
    "def mfcc(data, sr, flatten:bool=True):\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data,sr=22050, frame_length=2048, hop_length=512):\n",
    "    result=np.array([])\n",
    "    \n",
    "    result=np.hstack((result,\n",
    "                      zcr(data, frame_length, hop_length),\n",
    "                      rmse(data, frame_length, hop_length),\n",
    "                      mfcc(data, sr)\n",
    "                     ))\n",
    "    return result\n",
    "\n",
    "def get_features(path, duration=2.5, offset=0.6):\n",
    "    \n",
    "    normal_audio, sr = librosa.load(path, duration=duration, offset=offset)\n",
    "    \n",
    "    features_1 = extract_features(normal_audio)\n",
    "    features = np.array(features_1)\n",
    "    \n",
    "    noised_audio = noise(normal_audio)\n",
    "    noised_features = extract_features(noised_audio)\n",
    "    features = np.vstack((features, noised_features))\n",
    "    \n",
    "    pitched_audio = pitch(normal_audio, sr)\n",
    "    pitched_features = extract_features(pitched_audio)\n",
    "    features = np.vstack((features, pitched_features))\n",
    "    \n",
    "    pitched_audio_ = pitch(normal_audio, sr)\n",
    "    pitched_noised_audio = noise(pitched_audio_)\n",
    "    pitched_noised_features = extract_features(pitched_noised_audio)\n",
    "    features = np.vstack((features, pitched_noised_features))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "print('Starting to extract features...\\n')\n",
    "\n",
    "for path, emotion, index in tqdm (zip(df.file_path, df.emotion_id, range(df.file_path.shape[0]))):\n",
    "    \n",
    "    features = get_features(path)\n",
    "    \n",
    "    if (index % 1000 == 0):\n",
    "        print(f'{index} audios has been processed')\n",
    "    \n",
    "    for i in features:\n",
    "        X.append(i)\n",
    "        Y.append(emotion)\n",
    "\n",
    "print(f'Done! {len(X)} audios has been processed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
