{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification | Emotion Recognition in Speech**\n",
    "\n",
    "## **Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Useful functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(directories: list[str]):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        This function renames files, adding the \"author_##\" and keeping the id of the video\n",
    "\n",
    "    Args:\n",
    "        directories (list[str]): array of str, where each str is the local path to a directory, these directories contain videos .mp4\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting renaming process...\\n\")\n",
    "\n",
    "    for directory in directories:\n",
    "\n",
    "        print(f\"Renaming process for {directory} started.\")\n",
    "        \n",
    "        for filename in os.listdir(directory): # archives inside of the curret directory\n",
    "            if filename.endswith(\".wav\"): # if the file is a video with an _\n",
    "\n",
    "                id = filename.split(\"_\")[-1] # extract just the id\n",
    "                new_filename = f\"{directory.split(\"/\")[-1]}_{id}\" # concat just the id w '.mp4'\n",
    "\n",
    "                old_file = os.path.join(directory, filename) # path to the old file\n",
    "                new_file = os.path.join(directory, new_filename) # path to the new file\n",
    "\n",
    "                os.rename(old_file, new_file) # rename videos w just its id\n",
    "        \n",
    "        print(f\"Renaming process for {directory} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_audio_paths_txt(directories: list[str], path: list[str]):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        This function renames files, just keeping the id of the video\n",
    "\n",
    "    Args:\n",
    "        directories (list[str]): array of str, where each str is the local path to a directory, these directories contain videos .mp4\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting renaming process...\\n\")\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    for directory in range (len(directories)):\n",
    "\n",
    "        print(f\"Writing process for {directories[directory]} started.\")\n",
    "        \n",
    "        for filename in os.listdir(directories[directory]): # archives inside of the curret directory\n",
    "            if filename.endswith(\".wav\"): # if the file is a video with an _\n",
    "\n",
    "                name = filename # extract just the id\n",
    "                paths.append(os.path.join(path[directory], f\"{name}\")) # append the id to the array\n",
    "\n",
    "    print(paths)\n",
    "\n",
    "    output_file = f'./txt/paths.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        for path in paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the location according to your paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [f'./archive/Actor_{i:02d}' for i in range(1, 25)]\n",
    "paths = [f'D:/UTEC/IA/PROYECTO3/emotion-recognition-in-speech/archive/Actor_{i:02d}/' for i in range(1, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"txt\", exist_ok=True)\n",
    "\n",
    "rename_files(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_audio_paths_txt(directories, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos todos los path de los videos almacenados en un `.txt` (separado por train, val y test) podemos usar video_features para realizar la extracción de caracteristicas correspondiente.\n",
    "\n",
    "Para esto primero clonamos el repositorio de video_features e instalamos las dependencias necesarias dentro de este directorio/repositorio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/v-iashin/video_features.git\n",
    "cd video_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la instalación de dependencias, necesitas tener anaconda/miniconda instalado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda create -n video_features\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "conda install -c conda-forge omegaconf scipy tqdm pytest opencv\n",
    "conda install -c conda-forge av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto hecho, ya se puede realizar la extracción de caracteristicas en la terminal con el siguiente comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "python main.py \n",
    "        \\ feature_type=vggish\n",
    "        \\ device=\"cuda:0\" \n",
    "        \\ file_with_video_paths=\"../txt/paths.txt\" \n",
    "        \\ on_extraction=save_numpy \n",
    "        \\ output_path=\"../extraction\"\n",
    "\n",
    "# 'name' could be: [train, test, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando lo anterior, se crean archivos `.npy` dentro del directorio 'videos'. Cada archivo le corresponde a la extracción de características de un video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparacion de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificación de shapes en los archivos npy\n",
    "Se realiza la identificación de los shapes que se forman luego de obtener las caracteristicas de los videos, para encontrar la diferencia de los mismos y se pueda trabajar con una misma dimensionalidad en todos los videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de formas de los archivos .npy:\n",
      "Forma: (3, 128) -> Cantidad: 1054\n",
      "Forma: (4, 128) -> Cantidad: 379\n",
      "Forma: (5, 128) -> Cantidad: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Ruta de la carpeta donde están los archivos npy\n",
    "data_folder = \"./extraction/vggish/\"\n",
    "\n",
    "# Obtener la lista de archivos .npy en la carpeta\n",
    "npy_files = [f for f in os.listdir(data_folder) if f.endswith(\".npy\")]\n",
    "\n",
    "# Diccionario para almacenar las formas\n",
    "shape_counter = Counter()\n",
    "\n",
    "# Cargar los archivos y contar las formas\n",
    "for file in npy_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    data = np.load(file_path)\n",
    "    shape_counter[data.shape] += 1\n",
    "\n",
    "# Mostrar el conteo de formas únicas\n",
    "print(\"Conteo de formas de los archivos .npy:\")\n",
    "for shape, count in shape_counter.items():\n",
    "    print(f\"Forma: {shape} -> Cantidad: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar el label\n",
    "Se agrega el label correspondiente a la emoción para hacer el entrenamiento de los datos y su clasificación, no se modifica los archivos originales de extraction para mantenerlos sin modificaciones y poder utilizarlos para futuros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos transformados guardados en: ./label_added_npy/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta donde están los archivos .npy\n",
    "data_folder = \"./extraction/vggish/\"\n",
    "output_folder = \"./label_added_npy/\"\n",
    "\n",
    "# Crear carpeta si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Diccionario de emociones según el código en el nombre del archivo\n",
    "emotion_dict = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# Obtener la lista de archivos .npy\n",
    "npy_files = [f for f in os.listdir(data_folder) if f.endswith(\".npy\")]\n",
    "\n",
    "# Procesar archivos\n",
    "for file in npy_files:\n",
    "    parts = file.split('_')  # Separar por guiones bajos \"_\"\n",
    "    \n",
    "    if len(parts) >= 3:  # Asegurar que tiene suficiente estructura\n",
    "        emotion_code = parts[2].split('-')[2]  # Obtener el cuarto grupo (emoción)\n",
    "        emotion = emotion_dict.get(emotion_code, \"unknown\")  # Buscar en el diccionario\n",
    "        \n",
    "        # Cargar datos del archivo npy\n",
    "        data = np.load(os.path.join(data_folder, file))\n",
    "\n",
    "        # Guardar nuevo archivo con la etiqueta incluida como diccionario\n",
    "        output_path = os.path.join(output_folder, file)\n",
    "        np.save(output_path, {\"data\": data, \"label\": emotion})\n",
    "\n",
    "print(f\"Archivos transformados guardados en: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación por shapes\n",
    "Se crea una carpeta diferente para cada shape y se guarda cada video en su respectivo folder con su shape indicado para realizar un trabajo más eficiente con los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copiados 1054 archivos en: ./datasets_by_shape/shape_3_128\n",
      "Copiados 379 archivos en: ./datasets_by_shape/shape_4_128\n",
      "Copiados 7 archivos en: ./datasets_by_shape/shape_5_128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ruta de la carpeta donde están los archivos npy con label incluido\n",
    "data_folder = \"./label_added_npy\"\n",
    "output_folder = \"./datasets_by_shape/\"\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Obtener la lista de archivos .npy en la carpeta\n",
    "npy_files = [f for f in os.listdir(data_folder) if f.endswith(\".npy\")]\n",
    "\n",
    "# Diccionario para agrupar archivos por forma\n",
    "shape_dict = defaultdict(list)\n",
    "\n",
    "# Cargar los archivos y organizarlos por forma\n",
    "for file in npy_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    \n",
    "    # Cargar el archivo con allow_pickle=True porque ahora es un diccionario\n",
    "    loaded_data = np.load(file_path, allow_pickle=True).item()\n",
    "    \n",
    "    # Acceder a la parte de los datos\n",
    "    data_array = loaded_data[\"data\"]\n",
    "    \n",
    "    # Agrupar por forma\n",
    "    shape_dict[data_array.shape].append(file_path)\n",
    "\n",
    "# Copiar los archivos en su respectiva carpeta\n",
    "for shape, files in shape_dict.items():\n",
    "    shape_folder = os.path.join(output_folder, f\"shape_{'_'.join(map(str, shape))}\")\n",
    "    os.makedirs(shape_folder, exist_ok=True)\n",
    "\n",
    "    for file_path in files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        destination_path = os.path.join(shape_folder, filename)\n",
    "        shutil.copy(file_path, destination_path)\n",
    "\n",
    "    print(f\"Copiados {len(files)} archivos en: {shape_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacion en training y testing para cada uno de los Shapes\n",
    "Al momento de crear cada uno de los grupos e shapes, separamos cada grupo de datos en los training y testing para poder entrenar el modelo con informacion clara diferenciandose por las emociones y asegurandonos tener data de cada emocion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "División completada para shape_3_128\n",
      "División completada para shape_4_128\n",
      "División completada para shape_5_128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ruta de la carpeta donde están los archivos organizados por forma\n",
    "base_folder = \"./datasets_by_shape/\"\n",
    "\n",
    "# Recorrer todas las carpetas de forma\n",
    "for shape_folder in os.listdir(base_folder):\n",
    "    shape_path = os.path.join(base_folder, shape_folder)\n",
    "    \n",
    "    if not os.path.isdir(shape_path):  # Saltar archivos que no sean carpetas\n",
    "        continue\n",
    "\n",
    "    # Crear nueva carpeta de salida con el prefijo \"training_\"\n",
    "    new_folder = os.path.join(base_folder, f\"training_{shape_folder}\")\n",
    "    os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "    # Crear subcarpetas train y test dentro de la nueva carpeta\n",
    "    train_folder = os.path.join(new_folder, \"train\")\n",
    "    val_folder = os.path.join(new_folder, \"test\")\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "\n",
    "    # Obtener todos los archivos npy en la carpeta actual\n",
    "    npy_files = [f for f in os.listdir(shape_path) if f.endswith(\".npy\")]\n",
    "\n",
    "    # Diccionario para agrupar archivos por etiqueta\n",
    "    label_dict = {}\n",
    "\n",
    "    # Leer las etiquetas de cada archivo\n",
    "    for file in npy_files:\n",
    "        file_path = os.path.join(shape_path, file)\n",
    "        loaded_data = np.load(file_path, allow_pickle=True).item()\n",
    "        label = loaded_data[\"label\"]  # Obtener la etiqueta del diccionario\n",
    "\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = []\n",
    "        \n",
    "        label_dict[label].append(file_path)\n",
    "\n",
    "    # Dividir cada grupo en 70% train y 30% test (si hay más de 1 archivo)\n",
    "    for label, files in label_dict.items():\n",
    "        if len(files) == 1:\n",
    "            # Si solo hay un archivo, copiarlo a train directamente\n",
    "            shutil.copy(files[0], os.path.join(train_folder, os.path.basename(files[0])))\n",
    "        else:\n",
    "            # Si hay más de 1 archivo, dividirlo normalmente\n",
    "            train_files, val_files = train_test_split(files, test_size=0.3, random_state=42)\n",
    "\n",
    "            for file_path in train_files:\n",
    "                shutil.copy(file_path, os.path.join(train_folder, os.path.basename(file_path)))\n",
    "\n",
    "            for file_path in val_files:\n",
    "                shutil.copy(file_path, os.path.join(val_folder, os.path.basename(file_path)))\n",
    "\n",
    "    print(f\"División completada para {shape_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
